{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Qkr7-r3wsPOa"
      },
      "outputs": [],
      "source": [
        "# pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Byte-Pair Encoding**\n",
        "\n",
        "GPT-2, used BytePair encoding (BPE) as its tokenizer. It allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words."
      ],
      "metadata": {
        "id": "gpDbF3onRAJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **How does it achieve this without using <|unk|> tokens?**\n",
        "\n",
        "### The BPE algorithm, ensures that the most common words in vocabulary are represented as a single token, while rare words are breakdown into two or more subwords, if the tokenizer encounters an unfamiliar word during tokenization.\n",
        "\n",
        "### For instance, if GPT-2's vocabulary doesn't have the word \"analytical-nikita.io\", it might tokenize it as [\"analytical\", \"-\", \"nik\", \"ita\", \".\", \"io\"] or some other subword breakdown, depending on its trained BPE merges.\n",
        "\n",
        "### The original BPE tokenizer can be found here: https://github.com/openai/gpt-2/blob/master/src/encoder.py\n",
        "\n",
        "### Since, implementing BPE can be relatively complicated. In this demonstration, we are using the BPE tokenizer from OpenAI's open-source tiktoken library (https://github.com/openai/tiktoken), which implements its core algorithms in Rust to improve computational performance."
      ],
      "metadata": {
        "id": "Ag5x1ASrRGjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tiktoken is a fast BPE tokeniser for use with OpenAI's models."
      ],
      "metadata": {
        "id": "cSQ2WX10YFQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Dataset\n",
        "In this implementation I am using The Awakening by Kate Chopin, a public domain short story, written in 1899, so there is no copyright on that.\n",
        "\n",
        "Note: It is recommended to be aware and respectful of existing copyrights and people privacy, while preparing datasets for training LLMs."
      ],
      "metadata": {
        "id": "vnd1yU40Tvdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"pierre-pessarossi/tiny_shakespeare_dialogue\")\n",
        "train_data = dataset['train']\n"
      ],
      "metadata": {
        "id": "sqZeS3tZWlDM"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=train_data[\"text\"]\n"
      ],
      "metadata": {
        "id": "KkhC28R9XAV3"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "id": "cOmL5E3VXa6H",
        "outputId": "a4256e1b-cc5d-4e90-fa99-a88efb644847"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    <s>[INST] <<SYS>>\\n    You are William Shakespeare. Complete the following dialogue:\\n    <</SYS>>>\\n\\n    \\n\\nFirst Citizen:\\nBefore we proceed any further, hear me speak. [/INST] \\n\\nAll:\\nSpeak, speak. </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "# URL of the raw content of the markdown file\n",
        "if not os.path.exists(\"the-awkening.txt\"):\n",
        "    url = (\"https://raw.githubusercontent.com/mlschmitt/classic-books-markdown/main/Kate%20Chopin/The%20Awakening.md\")\n",
        "    file_path = \"the-awkening.txt\"\n",
        "    urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "with open(\"the-awkening.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    input_text = f.read()"
      ],
      "metadata": {
        "id": "yqi4MJK7Q63J"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing tiktoken"
      ],
      "metadata": {
        "id": "fZiLtk_UT2pG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ! pip3 install tiktoken"
      ],
      "metadata": {
        "id": "hqtp3O_HQ602"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_ZLljpQQ6no",
        "outputId": "506e33a9-6ef3-41d0-b844-1a3f07f78e91"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "6P-F04icT9es"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's instantiate a tokenizer object of SimpleTokenzier class and tokenize our sampled input text:"
      ],
      "metadata": {
        "id": "Vgxzt0YhUKx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text1 = \"Hello, you're learning data science with analytical-nikita.io\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1, text2))"
      ],
      "metadata": {
        "id": "XFBdDH-KT_Uk"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding\n",
        "This class have an encode method that splits text into tokens and carries out the string-to-integer mapping to produce token IDs via the vocabulary."
      ],
      "metadata": {
        "id": "ysrMmW8mUF7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "token_ids = tokenizer.encode(df[0], allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(token_ids)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gcs9vxaAUDja",
        "outputId": "468675e4-106e-4318-cc54-201f6343019f"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[198, 220, 220, 220, 1279, 82, 36937, 38604, 60, 9959, 50, 16309, 4211, 198, 220, 220, 220, 921, 389, 3977, 22197, 13, 13248, 262, 1708, 10721, 25, 198, 220, 220, 220, 1279, 3556, 50, 16309, 33409, 628, 220, 220, 220, 220, 198, 198, 5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 46581, 38604, 60, 220, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13, 7359, 82, 29]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The <|endoftext|> token is assigned a relatively large token ID, namely, 50256.\n",
        "\n",
        "In fact, the BPE tokenizer, which was used to train models such as GPT-2, GPT-3, and the original model used in ChatGPT, has a total vocabulary size of 50,257, with <|endoftext|> being assigned the largest token ID."
      ],
      "metadata": {
        "id": "uUp-wIq4UTms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Decoding\n",
        "We can then convert the token IDs back into text using the decode method, similar to our SimpleTokenizer that we build earlier:"
      ],
      "metadata": {
        "id": "xiva-CI8UPIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_strings = tokenizer.decode(token_ids)\n",
        "\n",
        "print(output_strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwjdXIQ-USdb",
        "outputId": "8be3cd47-27fd-4a19-e513-4e82b0fc1f81"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    <s>[INST] <<SYS>>\n",
            "    You are William Shakespeare. Complete the following dialogue:\n",
            "    <</SYS>>>\n",
            "\n",
            "    \n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak. [/INST] \n",
            "\n",
            "All:\n",
            "Speak, speak. </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The algorithm underlying BPE breaks down words like analytical-nikita.io that aren't in its predefined vocabulary into smaller subword units or even individual characters.\n",
        "\n",
        "The enables it to handle out-of-vocabulary words."
      ],
      "metadata": {
        "id": "qX7KZqKiUfp4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UGJQt2EAUeP1"
      },
      "execution_count": 83,
      "outputs": []
    }
  ]
}